data_hazards_list <- list(
      "automates-decision-making" = "Automated decision making can be hazardous for a number of reasons, and these will be highly dependent on the field in which it is being applied. We should ask ourselves whose decisions are being automated, what automation can bring to the process, and who is benefitted/harmed from this automation.",
      "classifies-people" = "Ranking and classifications of people are hazards in their own right and should be handled with care.",
      "difficult-to-understand" = "There is a danger that the technology is difficult to understand. This could be because of the technology itself is hard to interpret (e.g. neural nets), or problems with it’s implementation (i.e. code is not provided, or not documented).",
      "direct-harm" = "The application area of this technology means that it is capable of causing direct physical or psychological harm to someone even if used correctly e.g. healthcare and driverless vehicles may be expected to directly harm someone unless they have 100% accuracy.",
      "ecological-harm" = "This technology has the potential to cause broad ecological harm, even if used correctly.",
      "environment" = "This hazard is appropriate where methodologies are energy-hungry, data-hungry (requiring more and more computation), or require special hardware that require rare materials." ,
      "experimental-hazard" = "Translating technology into experimental practice can require safety precautions",
      "general-hazard" = "Data Science is being used in this output, and any negative outcome of using this work are not the fault of “the algorithm” or “the software”. This hazard applies to all Data Science research outputs.",
      "incompatible-data" = "Data of different types and/or sources are being used together that may not be compatible with each other.",
      "lacks-community" = "This applies when technology is being produced without input from the community it is supposed to serve.",
      "lacks-informed-consent" = "This hazard applies to datasets or algorithms that use data which has not been provided with the explicit consent of the data owner/creator. This data often lacks other contextual information which can also make it difficult to understand how the dataset may be biased.",
      "misuse" = "There is a danger of misusing the algorithm, technology, or data collected as part of this work.",
      "privacy" = "This technology may risk the privacy of individuals whose data is processed by it.",
      "reinforce-bias" = "Reinforces unfair treatment of individuals and groups. This may be due to for example input data, algorithm or software design choices, or society at large.",
      "uncertain-accuracy" = "The accuracy of the underlying data is not known and so its use may lead to erroneous results or introduce bias.",
      "uncertain-completeness" = "Underlying data is of an uncertain completeness and have missing values that causes biased results."
    )